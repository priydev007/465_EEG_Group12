{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Project Group 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Data and Performing Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here upto and including EEG Feature Extraction was taken from the sample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priyd\\Downloads\\465_EEG_Group12\n",
      "                     TimeStamp  Delta_TP9  Delta_AF7  Delta_AF8  Delta_TP10  \\\n",
      "0      2024-11-21 14:04:16.799        NaN        NaN        NaN         NaN   \n",
      "1      2024-11-21 14:04:16.829   1.142701   0.842652   0.237114    1.623223   \n",
      "2      2024-11-21 14:04:16.829   1.142701   0.842652   0.237114    1.623223   \n",
      "3      2024-11-21 14:04:16.829   1.142701   0.842652   0.237114    1.623223   \n",
      "4      2024-11-21 14:04:16.830   1.142701   0.842652   0.237114    1.623223   \n",
      "...                        ...        ...        ...        ...         ...   \n",
      "17760  2024-11-21 14:05:25.887   0.743850   0.394492   1.152595    0.889482   \n",
      "17761  2024-11-21 14:05:25.887   0.743850   0.394492   1.152595    0.889482   \n",
      "17762  2024-11-21 14:05:25.887   0.743850   0.394492   1.152595    0.889482   \n",
      "17763  2024-11-21 14:05:25.887   0.743850   0.394492   1.152595    0.889482   \n",
      "17764  2024-11-21 14:05:25.887   0.743850   0.394492   1.152595    0.889482   \n",
      "\n",
      "       Theta_TP9  Theta_AF7  Theta_AF8  Theta_TP10  Alpha_TP9  ...   Gyro_X  \\\n",
      "0            NaN        NaN        NaN         NaN        NaN  ...      NaN   \n",
      "1       0.602743   0.191248   0.007583    1.703722   0.233287  ...  3.39447   \n",
      "2       0.602743   0.191248   0.007583    1.703722   0.233287  ...  3.39447   \n",
      "3       0.602743   0.191248   0.007583    1.703722   0.233287  ...  3.39447   \n",
      "4       0.602743   0.191248   0.007583    1.703722   0.233287  ...  3.39447   \n",
      "...          ...        ...        ...         ...        ...  ...      ...   \n",
      "17760   0.575718   0.232133   0.394247   -0.269688   0.402307  ... -4.63562   \n",
      "17761   0.575718   0.232133   0.394247   -0.269688   0.402307  ... -4.63562   \n",
      "17762   0.575718   0.232133   0.394247   -0.269688   0.402307  ... -4.63562   \n",
      "17763   0.575718   0.232133   0.394247   -0.269688   0.402307  ... -4.63562   \n",
      "17764   0.575718   0.232133   0.394247   -0.269688   0.402307  ... -4.63562   \n",
      "\n",
      "          Gyro_Y    Gyro_Z  HeadBandOn  HSI_TP9  HSI_AF7  HSI_AF8  HSI_TP10  \\\n",
      "0            NaN       NaN         NaN      NaN      NaN      NaN       NaN   \n",
      "1      -9.443207 -0.575714         1.0      2.0      1.0      1.0       2.0   \n",
      "2      -9.443207 -0.575714         1.0      2.0      1.0      1.0       2.0   \n",
      "3      -9.443207 -0.575714         1.0      2.0      1.0      1.0       2.0   \n",
      "4      -9.443207 -0.575714         1.0      2.0      1.0      1.0       2.0   \n",
      "...          ...       ...         ...      ...      ...      ...       ...   \n",
      "17760 -13.241425 -1.315918         1.0      2.0      1.0      1.0       2.0   \n",
      "17761 -13.241425 -1.315918         1.0      2.0      1.0      1.0       2.0   \n",
      "17762 -13.241425 -1.315918         1.0      2.0      1.0      1.0       2.0   \n",
      "17763 -13.241425 -1.315918         1.0      2.0      1.0      1.0       2.0   \n",
      "17764 -13.241425 -1.315918         1.0      2.0      1.0      1.0       2.0   \n",
      "\n",
      "       Battery                         Elements  \n",
      "0          NaN  /muse/event/connected Muse-9E89  \n",
      "1         90.0                              NaN  \n",
      "2         90.0                              NaN  \n",
      "3         90.0                              NaN  \n",
      "4         90.0                              NaN  \n",
      "...        ...                              ...  \n",
      "17760     90.0                              NaN  \n",
      "17761     90.0                              NaN  \n",
      "17762     90.0                              NaN  \n",
      "17763     90.0                              NaN  \n",
      "17764     90.0                              NaN  \n",
      "\n",
      "[17765 rows x 39 columns]\n",
      "                     TimeStamp  Delta_TP9  Delta_AF7  Delta_AF8  Delta_TP10  \\\n",
      "0      2024-11-21 13:55:13.012        NaN        NaN        NaN         NaN   \n",
      "1      2024-11-21 13:55:13.049    0.00000   1.017581   1.494045    0.000000   \n",
      "2      2024-11-21 13:55:13.050    0.00000   1.017581   1.494045    0.000000   \n",
      "3      2024-11-21 13:55:13.050    0.00000   1.017581   1.494045    0.000000   \n",
      "4      2024-11-21 13:55:13.050    0.00000   1.017581   1.494045    0.000000   \n",
      "...                        ...        ...        ...        ...         ...   \n",
      "20255  2024-11-21 13:56:31.763    1.07758   0.489131   1.025969    0.445634   \n",
      "20256  2024-11-21 13:56:31.763    1.07758   0.489131   1.025969    0.445634   \n",
      "20257  2024-11-21 13:56:31.763    1.07758   0.489131   1.025969    0.445634   \n",
      "20258  2024-11-21 13:56:31.764    1.07758   0.489131   1.025969    0.445634   \n",
      "20259  2024-11-21 13:56:31.764    1.07758   0.489131   1.025969    0.445634   \n",
      "\n",
      "       Theta_TP9  Theta_AF7  Theta_AF8  Theta_TP10  Alpha_TP9  ...    Gyro_X  \\\n",
      "0            NaN        NaN        NaN         NaN        NaN  ...       NaN   \n",
      "1       0.000000   1.027297   0.937403    0.000000   0.000000  ...  1.405640   \n",
      "2       0.000000   1.027297   0.937403    0.000000   0.000000  ...  1.405640   \n",
      "3       0.000000   1.027297   0.937403    0.000000   0.000000  ...  1.405640   \n",
      "4       0.000000   1.027297   0.937403    0.000000   0.000000  ...  1.405640   \n",
      "...          ...        ...        ...         ...        ...  ...       ...   \n",
      "20255   0.616907   0.129927   0.701287    0.247421   0.429564  ...  2.422485   \n",
      "20256   0.616907   0.129927   0.701287    0.247421   0.429564  ...  2.422485   \n",
      "20257   0.616907   0.129927   0.701287    0.247421   0.429564  ...  2.422485   \n",
      "20258   0.616907   0.129927   0.701287    0.247421   0.429564  ...  2.422485   \n",
      "20259   0.616907   0.129927   0.701287    0.247421   0.429564  ...  2.422485   \n",
      "\n",
      "        Gyro_Y    Gyro_Z  HeadBandOn  HSI_TP9  HSI_AF7  HSI_AF8  HSI_TP10  \\\n",
      "0          NaN       NaN         NaN      NaN      NaN      NaN       NaN   \n",
      "1     -1.42807  2.519684         1.0      4.0      1.0      1.0       4.0   \n",
      "2     -1.42807  2.519684         1.0      4.0      1.0      1.0       4.0   \n",
      "3     -1.42807  2.519684         1.0      4.0      1.0      1.0       4.0   \n",
      "4     -1.42807  2.519684         1.0      4.0      1.0      1.0       4.0   \n",
      "...        ...       ...         ...      ...      ...      ...       ...   \n",
      "20255 -1.61499  4.366455         1.0      2.0      1.0      1.0       4.0   \n",
      "20256 -1.61499  4.366455         1.0      2.0      1.0      1.0       4.0   \n",
      "20257 -1.61499  4.366455         1.0      2.0      1.0      1.0       4.0   \n",
      "20258 -1.61499  4.366455         1.0      2.0      1.0      1.0       4.0   \n",
      "20259 -1.61499  4.366455         1.0      2.0      1.0      1.0       4.0   \n",
      "\n",
      "       Battery                         Elements  \n",
      "0          NaN  /muse/event/connected Muse-9E89  \n",
      "1         95.0                              NaN  \n",
      "2         95.0                              NaN  \n",
      "3         95.0                              NaN  \n",
      "4         95.0                              NaN  \n",
      "...        ...                              ...  \n",
      "20255     95.0                              NaN  \n",
      "20256     95.0                              NaN  \n",
      "20257     95.0                              NaN  \n",
      "20258     95.0                              NaN  \n",
      "20259     95.0                              NaN  \n",
      "\n",
      "[20260 rows x 39 columns]\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 333 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Get Current Working directory and append the data relative dir\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "Salt = cwd + r\"/data/salt\"\n",
    "Sour = cwd + r\"/data/sour\"\n",
    "\n",
    "#Hold file locations\n",
    "filesSour=[];\n",
    "filesSalt=[]; #add more later\n",
    "\n",
    "\n",
    "#populate file location arrays\n",
    "for file in os.listdir(Salt):\n",
    "    if file.endswith('csv'):\n",
    "        filesSalt.append(os.path.join(Salt,file))\n",
    "for file in os.listdir(Sour):\n",
    "    if file.endswith('csv'):\n",
    "        filesSour.append(os.path.join(Sour,file))\n",
    "#Test reading files by changing num\n",
    "\n",
    "num = 5;\n",
    "Salt_sample = pd.read_csv(filesSalt[num])\n",
    "Sour_sample =  pd.read_csv(filesSour[num])\n",
    "print(Salt_sample)\n",
    "print(Sour_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">6 files were added from the Salty category\n"
     ]
    }
   ],
   "source": [
    "#Mini-Summary of Block\n",
    "#print(f\">{len(filesSpicy)} files were added from the Spicy category\")\n",
    "print(f\">{len(filesSalt)} files were added from the Salty category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features generated by the Muse 2 headband:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TimeStamp</td>\n",
       "      <td>Delta_TP9</td>\n",
       "      <td>Delta_AF7</td>\n",
       "      <td>Delta_AF8</td>\n",
       "      <td>Delta_TP10</td>\n",
       "      <td>Theta_TP9</td>\n",
       "      <td>Theta_AF7</td>\n",
       "      <td>Theta_AF8</td>\n",
       "      <td>Theta_TP10</td>\n",
       "      <td>Alpha_TP9</td>\n",
       "      <td>...</td>\n",
       "      <td>Gyro_X</td>\n",
       "      <td>Gyro_Y</td>\n",
       "      <td>Gyro_Z</td>\n",
       "      <td>HeadBandOn</td>\n",
       "      <td>HSI_TP9</td>\n",
       "      <td>HSI_AF7</td>\n",
       "      <td>HSI_AF8</td>\n",
       "      <td>HSI_TP10</td>\n",
       "      <td>Battery</td>\n",
       "      <td>Elements</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1          2          3           4          5   \\\n",
       "0  TimeStamp  Delta_TP9  Delta_AF7  Delta_AF8  Delta_TP10  Theta_TP9   \n",
       "\n",
       "          6          7           8          9   ...      29      30      31  \\\n",
       "0  Theta_AF7  Theta_AF8  Theta_TP10  Alpha_TP9  ...  Gyro_X  Gyro_Y  Gyro_Z   \n",
       "\n",
       "           32       33       34       35        36       37        38  \n",
       "0  HeadBandOn  HSI_TP9  HSI_AF7  HSI_AF8  HSI_TP10  Battery  Elements  \n",
       "\n",
       "[1 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Features generated by the Muse 2 headband:\")\n",
    "pd.DataFrame(Salt_sample.columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RAW Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:10\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\priyd\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\priyd\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\priyd\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\priyd\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    %%time\n",
    "    #Extract rows 21-25 from all files,\n",
    "    #these are the only 5 features relevant for use in the EEG_feature_extraction function\n",
    "    #rowsSpicy=[];\n",
    "    #for f in filesSpicy:\n",
    "    #   for r in range(pd.read_csv(f).shape[0]):\n",
    "    #       rowsSpicy.append(pd.read_csv(f).iloc[r,[0,21,22,23,24,25]])\n",
    "    rowsSalty=[];\n",
    "    for f in filesSalt:\n",
    "        for r in range(pd.read_csv(f).shape[0]):\n",
    "            rowsSalty.append(pd.read_csv(f).iloc[r,[0,21,22,23,24,25]])\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowsSalty = []\n",
    "\n",
    "for f in filesSalt:\n",
    "    df = pd.read_csv(f)  # Read the file once\n",
    "    rowsSalty.extend(df.iloc[:, [0, 21, 22, 23, 24, 25]].values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to DataFrames\n",
    "data_Spicy = pd.DataFrame(rowsSpicy);\n",
    "original_Spicy = data_Spicy.copy();\n",
    "data_Spicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-21 02:05:38.593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-21 02:05:38.626</td>\n",
       "      <td>796.996337</td>\n",
       "      <td>806.666667</td>\n",
       "      <td>829.230769</td>\n",
       "      <td>589.890110</td>\n",
       "      <td>766.373626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-21 02:05:38.626</td>\n",
       "      <td>817.948718</td>\n",
       "      <td>778.864469</td>\n",
       "      <td>826.007326</td>\n",
       "      <td>116.446886</td>\n",
       "      <td>728.498168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-21 02:05:38.626</td>\n",
       "      <td>809.487179</td>\n",
       "      <td>803.846154</td>\n",
       "      <td>839.304029</td>\n",
       "      <td>857.032967</td>\n",
       "      <td>741.391941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-21 02:05:38.627</td>\n",
       "      <td>802.637363</td>\n",
       "      <td>812.307692</td>\n",
       "      <td>840.915751</td>\n",
       "      <td>1467.069597</td>\n",
       "      <td>792.564103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191587</th>\n",
       "      <td>2024-11-21 14:05:25.887</td>\n",
       "      <td>772.417582</td>\n",
       "      <td>764.761905</td>\n",
       "      <td>884.029304</td>\n",
       "      <td>867.106227</td>\n",
       "      <td>803.040293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191588</th>\n",
       "      <td>2024-11-21 14:05:25.887</td>\n",
       "      <td>753.882784</td>\n",
       "      <td>772.014652</td>\n",
       "      <td>884.432234</td>\n",
       "      <td>850.183150</td>\n",
       "      <td>593.516484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191589</th>\n",
       "      <td>2024-11-21 14:05:25.887</td>\n",
       "      <td>741.391941</td>\n",
       "      <td>761.941392</td>\n",
       "      <td>869.120879</td>\n",
       "      <td>836.483516</td>\n",
       "      <td>606.813187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191590</th>\n",
       "      <td>2024-11-21 14:05:25.887</td>\n",
       "      <td>745.018315</td>\n",
       "      <td>757.106227</td>\n",
       "      <td>869.120879</td>\n",
       "      <td>857.032967</td>\n",
       "      <td>701.098901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191591</th>\n",
       "      <td>2024-11-21 14:05:25.887</td>\n",
       "      <td>747.838828</td>\n",
       "      <td>764.761905</td>\n",
       "      <td>881.611722</td>\n",
       "      <td>855.824176</td>\n",
       "      <td>600.366300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191592 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0           1           2           3  \\\n",
       "0       2024-11-21 02:05:38.593         NaN         NaN         NaN   \n",
       "1       2024-11-21 02:05:38.626  796.996337  806.666667  829.230769   \n",
       "2       2024-11-21 02:05:38.626  817.948718  778.864469  826.007326   \n",
       "3       2024-11-21 02:05:38.626  809.487179  803.846154  839.304029   \n",
       "4       2024-11-21 02:05:38.627  802.637363  812.307692  840.915751   \n",
       "...                         ...         ...         ...         ...   \n",
       "191587  2024-11-21 14:05:25.887  772.417582  764.761905  884.029304   \n",
       "191588  2024-11-21 14:05:25.887  753.882784  772.014652  884.432234   \n",
       "191589  2024-11-21 14:05:25.887  741.391941  761.941392  869.120879   \n",
       "191590  2024-11-21 14:05:25.887  745.018315  757.106227  869.120879   \n",
       "191591  2024-11-21 14:05:25.887  747.838828  764.761905  881.611722   \n",
       "\n",
       "                  4           5  \n",
       "0               NaN         NaN  \n",
       "1        589.890110  766.373626  \n",
       "2        116.446886  728.498168  \n",
       "3        857.032967  741.391941  \n",
       "4       1467.069597  792.564103  \n",
       "...             ...         ...  \n",
       "191587   867.106227  803.040293  \n",
       "191588   850.183150  593.516484  \n",
       "191589   836.483516  606.813187  \n",
       "191590   857.032967  701.098901  \n",
       "191591   855.824176  600.366300  \n",
       "\n",
       "[191592 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert to DataFrames\n",
    "data_Salty = pd.DataFrame(rowsSalty);\n",
    "original_Salty = data_Salty.copy();\n",
    "data_Salty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Salty data size is: \t(191592, 6)\n"
     ]
    }
   ],
   "source": [
    "#quick check of DataFrames\n",
    "#print(f\"Spicy data size is: \\t{data_Spicy.shape}\",f\"\\nSalty data size is: \\t{data_Salty.shape}\")\n",
    "print(f\"\\nSalty data size is: \\t{data_Salty.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Empty Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove NaN values\n",
    "#data_Spicy = data_Spicy.dropna()\n",
    "data_Salty = data_Salty.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Datetime Column to Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required for compatibility with EEG_feature_extraction function\n",
    "from datetime import datetime\n",
    "\"\"\"\n",
    "    i = 0;\n",
    "    for time in data_Spicy.iloc[:,0]:\n",
    "        tmstmp = datetime.strptime(str(time),'%Y-%m-%d %H:%M:%S.%f').timestamp()\n",
    "        data_Spicy.iat[i,0] = (tmstmp);\n",
    "    i=i+1;\n",
    "\"\"\"\n",
    "i = 0;\n",
    "for time in data_Salty.iloc[:,0]:\n",
    "    tmstmp = datetime.strptime(str(time),'%Y-%m-%d %H:%M:%S.%f').timestamp()\n",
    "    data_Salty.iat[i,0] = (tmstmp);\n",
    "i=i+1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_Spicy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Quick Check\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdata_Spicy\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_Spicy' is not defined"
     ]
    }
   ],
   "source": [
    "#Quick Check\n",
    "data_Spicy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1732226725.887</td>\n",
       "      <td>796.996337</td>\n",
       "      <td>806.666667</td>\n",
       "      <td>829.230769</td>\n",
       "      <td>589.890110</td>\n",
       "      <td>766.373626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-21 02:05:38.626</td>\n",
       "      <td>817.948718</td>\n",
       "      <td>778.864469</td>\n",
       "      <td>826.007326</td>\n",
       "      <td>116.446886</td>\n",
       "      <td>728.498168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-21 02:05:38.626</td>\n",
       "      <td>809.487179</td>\n",
       "      <td>803.846154</td>\n",
       "      <td>839.304029</td>\n",
       "      <td>857.032967</td>\n",
       "      <td>741.391941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-21 02:05:38.627</td>\n",
       "      <td>802.637363</td>\n",
       "      <td>812.307692</td>\n",
       "      <td>840.915751</td>\n",
       "      <td>1467.069597</td>\n",
       "      <td>792.564103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-11-21 02:05:38.627</td>\n",
       "      <td>794.981685</td>\n",
       "      <td>772.014652</td>\n",
       "      <td>820.769231</td>\n",
       "      <td>858.241758</td>\n",
       "      <td>838.095238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0           1           2           3            4  \\\n",
       "1           1732226725.887  796.996337  806.666667  829.230769   589.890110   \n",
       "2  2024-11-21 02:05:38.626  817.948718  778.864469  826.007326   116.446886   \n",
       "3  2024-11-21 02:05:38.626  809.487179  803.846154  839.304029   857.032967   \n",
       "4  2024-11-21 02:05:38.627  802.637363  812.307692  840.915751  1467.069597   \n",
       "5  2024-11-21 02:05:38.627  794.981685  772.014652  820.769231   858.241758   \n",
       "\n",
       "            5  \n",
       "1  766.373626  \n",
       "2  728.498168  \n",
       "3  741.391941  \n",
       "4  792.564103  \n",
       "5  838.095238  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Salty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data to File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative STARTING POINT once data collection is finalized. This step was done to bypass having to run the previous section each time which would take a very long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priyd\\AppData\\Local\\Temp\\ipykernel_162380\\32171580.py:13: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(savelocSalty, header=None, on_bad_lines='skip')  # Skip bad lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the cleaned file:\n",
      "                      col1               col2                col3       col4  \\\n",
      "0                TimeStamp          Delta_TP9           Delta_AF7  Delta_AF8   \n",
      "1  2024-11-21 02:13:15.650                NaN                 NaN        NaN   \n",
      "2  2024-11-21 02:13:15.679  1.353650944890756  0.8681295123233584          0   \n",
      "3  2024-11-21 02:13:15.680  1.353650944890756  0.8681295123233584          0   \n",
      "4  2024-11-21 02:13:15.680  1.353650944890756  0.8681295123233584          0   \n",
      "\n",
      "                 col5               col6                col7       col8  \\\n",
      "0          Delta_TP10          Theta_TP9           Theta_AF7  Theta_AF8   \n",
      "1                 NaN                NaN                 NaN        NaN   \n",
      "2  0.7058293529551414  1.230388383342542  0.5419058172714343          0   \n",
      "3  0.7058293529551414  1.230388383342542  0.5419058172714343          0   \n",
      "4  0.7058293529551414  1.230388383342542  0.5419058172714343          0   \n",
      "\n",
      "                 col9               col10  ...     col30     col31      col32  \\\n",
      "0          Theta_TP10           Alpha_TP9  ...    Gyro_X    Gyro_Y     Gyro_Z   \n",
      "1                 NaN                 NaN  ...       NaN       NaN        NaN   \n",
      "2  0.4835286104617367  0.6889696575156116  ...  3.349609  1.271057  -0.687866   \n",
      "3  0.4835286104617367  0.6889696575156116  ...  3.349609  1.271057  -0.687866   \n",
      "4  0.4835286104617367  0.6889696575156116  ...  3.349609  1.271057  -0.687866   \n",
      "\n",
      "        col33    col34    col35    col36     col37      col38  \\\n",
      "0  HeadBandOn  HSI_TP9  HSI_AF7  HSI_AF8  HSI_TP10    Battery   \n",
      "1         NaN      NaN      NaN      NaN       NaN        NaN   \n",
      "2           1        1        1        1         1  15.000000   \n",
      "3           1        1        1        1         1  15.000000   \n",
      "4           1        1        1        1         1  15.000000   \n",
      "\n",
      "                             col39  \n",
      "0                         Elements  \n",
      "1  /muse/event/connected Muse-9E89  \n",
      "2                              NaN  \n",
      "3                              NaN  \n",
      "4                              NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "\n",
      "Number of columns: 39\n",
      "\n",
      "Cleaned file saved to: C:\\Users\\priyd\\Downloads\\465_EEG_Group12\\data\\salt\\2_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# savelocSalty = r\"C:\\Users\\priyd\\Downloads\\465_EEG_Group12\\data\\salt\\2.csv\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "savelocSalty = r\"C:\\Users\\priyd\\Downloads\\465_EEG_Group12\\data\\salt\\2.csv\"\n",
    "\n",
    "# Ensure the file exists before proceeding\n",
    "if os.path.exists(savelocSalty):\n",
    "    try:\n",
    "        # Attempt to read the file, skipping problematic rows\n",
    "        df = pd.read_csv(savelocSalty, header=None, on_bad_lines='skip')  # Skip bad lines\n",
    "        \n",
    "        # Assign column names\n",
    "        df.columns = [f\"col{i}\" for i in range(1, len(df.columns) + 1)]\n",
    "        \n",
    "        # Save cleaned DataFrame back to a new file\n",
    "        cleaned_file = savelocSalty.replace(\".csv\", \"_cleaned.csv\")\n",
    "        df.to_csv(cleaned_file, index=False)\n",
    "        \n",
    "        # Display preview of the cleaned DataFrame\n",
    "        print(\"Preview of the cleaned file:\")\n",
    "        print(df.head())\n",
    "        print(f\"\\nNumber of columns: {len(df.columns)}\")\n",
    "        print(f\"\\nCleaned file saved to: {cleaned_file}\")\n",
    "        \n",
    "    except pd.errors.ParserError as e:\n",
    "        # Log error if parsing fails\n",
    "        print(f\"ParserError occurred: {e}\")\n",
    "        \n",
    "        # Manually inspect problematic rows\n",
    "        print(\"\\nIdentifying problematic rows:\")\n",
    "        with open(savelocSalty, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for i, line in enumerate(lines):\n",
    "                if len(line.split(',')) != len(lines[0].split(',')):  # Compare row structure\n",
    "                    print(f\"Issue at line {i + 1}: {line}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(f\"File not found: {savelocSalty}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved at: data/data1_with_headers.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\priyd\\Downloads\\465_EEG_Group12\\eeg_feature_generation\")  # Use raw string to handle backslashes\n",
    "from eeg_feature_generation import EEG_feature_extraction as FG\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEG Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#try various combinations of Nsamp and Perio\n",
    "Nsamp = 50;\n",
    "Perio = 6;\n",
    "\n",
    "xSpicy,ySpicy = FG.generate_feature_vectors_from_samples(file_path=savelocSpicy,\n",
    "                                                         nsamples=Nsamp,\n",
    "                                                         period=Perio,\n",
    "                                                         #state=data_Spicy.iloc[:,-1],\n",
    "                                                         slide_percent=0.05,\n",
    "                                                         remove_redundant=False,\n",
    "                                                         cols_to_ignore=None)\n",
    "xSpicy.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priyd\\AppData\\Local\\Temp\\ipykernel_162380\\990492595.py:2: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(savelocSalty, on_bad_lines='skip', header=None)\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare the CSV for feature generation\n",
    "df = pd.read_csv(savelocSalty, on_bad_lines='skip', header=None)\n",
    "df.columns = [f\"col{i}\" for i in range(1, len(df.columns) + 1)]  # Assign generic headers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved to: C:\\Users\\priyd\\Downloads\\465_EEG_Group12\\data\\salt\\2_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare the dataset for feature generation\n",
    "try:\n",
    "    # Read the sample data\n",
    "    df = pd.read_csv(savelocSalty, dtype=str, on_bad_lines='skip')  # Load all as strings for inspection\n",
    "\n",
    "    # Drop irrelevant columns (e.g., \"Elements\")\n",
    "    if \"Elements\" in df.columns:\n",
    "        df.drop(columns=[\"Elements\"], inplace=True)\n",
    "\n",
    "    # Convert columns to numeric where applicable\n",
    "    for col in df.columns[1:]:  # Assuming the first column is 'TimeStamp'\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Drop rows with all NaN feature values\n",
    "    df.dropna(subset=df.columns[1:], how='all', inplace=True)\n",
    "\n",
    "    # Drop rows where TimeStamp or other critical columns are NaN\n",
    "    df.dropna(subset=[\"TimeStamp\"], inplace=True)\n",
    "\n",
    "    # Validate the dataset\n",
    "    if df.shape[0] == 0 or df.shape[1] < 5:\n",
    "        raise ValueError(\"Insufficient data for processing. Check input dataset.\")\n",
    "\n",
    "    # Save cleaned data\n",
    "    cleaned_file = savelocSalty.replace(\".csv\", \"_cleaned.csv\")\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "    print(f\"Cleaned file saved to: {cleaned_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while cleaning data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError during feature generation: zero-size array to reduction operation maximum which has no identity\n"
     ]
    }
   ],
   "source": [
    "# Generate features from the cleaned file\n",
    "try:\n",
    "    # Ensure sufficient rows in the cleaned file\n",
    "    cleaned_df = pd.read_csv(cleaned_file)\n",
    "    if cleaned_df.shape[0] < 50:  # Replace 50 with the minimum number of rows required for generation\n",
    "        raise ValueError(\"Insufficient rows in the cleaned file for feature generation.\")\n",
    "\n",
    "    # Perform feature generation\n",
    "    xSalty, ySalty = FG.generate_feature_vectors_from_samples(\n",
    "        file_path=cleaned_file,\n",
    "        nsamples=50,  # Example value\n",
    "        period=5,     # Example value\n",
    "        remove_redundant=False,\n",
    "        cols_to_ignore=None\n",
    "    )\n",
    "    print(f\"Shape of generated features (xSalty): {xSalty.shape}\")\n",
    "except ValueError as ve:\n",
    "    print(f\"ValueError during feature generation: {ve}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during feature generation: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m Nsamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      6\u001b[0m Perio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 8\u001b[0m xSalty, ySalty \u001b[38;5;241m=\u001b[39m \u001b[43mFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_feature_vectors_from_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleaned_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNsamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPerio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_redundant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcols_to_ignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of xSalty:\u001b[39m\u001b[38;5;124m\"\u001b[39m, xSalty\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\priyd\\Downloads\\465_EEG_Group12\\eeg_feature_generation\\EEG_feature_extraction.py:847\u001b[0m, in \u001b[0;36mgenerate_feature_vectors_from_samples\u001b[1;34m(file_path, nsamples, period, state, remove_redundant, cols_to_ignore)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    842\u001b[0m \t\u001b[38;5;66;03m# Get the next slice from the file (starting at time 't', with a \u001b[39;00m\n\u001b[0;32m    843\u001b[0m \t\u001b[38;5;66;03m# duration of 'period'\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \t\u001b[38;5;66;03m# If an exception is raised or the slice is not as long as we expected, \u001b[39;00m\n\u001b[0;32m    845\u001b[0m \t\u001b[38;5;66;03m# return the current data available\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \t\u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 847\u001b[0m \t\ts, dur \u001b[38;5;241m=\u001b[39m \u001b[43mget_time_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    848\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m cols_to_ignore \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    849\u001b[0m \t\t\ts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(s, cols_to_ignore, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\priyd\\Downloads\\465_EEG_Group12\\eeg_feature_generation\\EEG_feature_extraction.py:69\u001b[0m, in \u001b[0;36mget_time_slice\u001b[1;34m(full_matrix, start, period)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Changed for greater efficiency [fcampelo]\u001b[39;00m\n\u001b[0;32m     68\u001b[0m rstart  \u001b[38;5;241m=\u001b[39m full_matrix[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m start\n\u001b[1;32m---> 69\u001b[0m index_0 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrstart\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m index_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mwhere(full_matrix[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rstart \u001b[38;5;241m+\u001b[39m period))\n\u001b[0;32m     72\u001b[0m duration \u001b[38;5;241m=\u001b[39m full_matrix[index_1, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m full_matrix[index_0, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\priyd\\anaconda3\\envs\\tf_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\priyd\\anaconda3\\envs\\tf_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "#try various combinations of Nsamp and Perio\n",
    "\n",
    "\n",
    "\n",
    "Nsamp = 50\n",
    "Perio = 5\n",
    "\n",
    "xSalty, ySalty = FG.generate_feature_vectors_from_samples(\n",
    "    file_path=cleaned_file,\n",
    "    nsamples=Nsamp,\n",
    "    period=Perio,\n",
    "    remove_redundant=False,\n",
    "    cols_to_ignore=None\n",
    ")\n",
    "\n",
    "print(\"Shape of xSalty:\", xSalty.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some quick checks\n",
    "X_Spicy = pd.DataFrame(np.real(xSpicy))\n",
    "X_Spicy.columns = np.hstack((['Timestamp'],ySpicy))\n",
    "X_Spicy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows',None,'display.max_columns',None):\n",
    "    display(pd.DataFrame(pd.DataFrame(X_Spicy).head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Salty = pd.DataFrame(np.real(xSalty))\n",
    "X_Salty.columns = np.hstack((['Timestamp'],ySalty))\n",
    "X_Salty.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Into Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,-1].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_standardized = StandardScaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 225)\n",
    "X_pca = pca.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_standardized = StandardScaler.fit_transform(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(10,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(5,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs, y_train, batch_size = 32, epochs = 100, validation_split=0.2, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
